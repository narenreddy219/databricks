{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------\n",
    "CONFIG = {\n",
    "    \"aws_access_key\": \"<your_access_key>\",\n",
    "    \"aws_secret_key\": \"<your_secret_key>\",\n",
    "    \"aws_session_token\": \"<your_session_token>\",  # Optional\n",
    "    \"landing_zone\": \"s3://your-bucket/landing-zone/\",\n",
    "    \"archive_zone\": \"s3://your-bucket/archive-zone/\",\n",
    "    \"checkpoint_path\": \"/tmp/autoloader/checkpoints\",\n",
    "    \"catalog\": \"entity_resolution_dev\",\n",
    "    \"schema\": \"bronze\"\n",
    "}\n",
    "\n",
    "# Set AWS credentials in environment\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = CONFIG[\"aws_access_key\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = CONFIG[\"aws_secret_key\"]\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = CONFIG.get(\"aws_session_token\", \"\")\n",
    "\n",
    "# ---------------------------\n",
    "# BOTO3 & SPARK SETUP\n",
    "# ---------------------------\n",
    "s3 = boto3.client(\"s3\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ---------------------------\n",
    "# HELPERS\n",
    "# ---------------------------\n",
    "def get_valid_tables(catalog, schema):\n",
    "    return [t.name for t in spark.catalog.listTables(f\"{catalog}.{schema}\")]\n",
    "\n",
    "def extract_table_name(file_path, valid_tables):\n",
    "    file_name = os.path.basename(file_path).split('.')[0]\n",
    "    for table in sorted(valid_tables, key=len, reverse=True):\n",
    "        if file_name.startswith(table):\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "def list_s3_files(bucket, prefix):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if not key.endswith(\"/\"):\n",
    "                yield f\"s3://{bucket}/{key}\"\n",
    "\n",
    "def move_s3_file(source_uri, destination_uri):\n",
    "    src_bucket, src_key = re.match(r\"s3://([^/]+)/(.+)\", source_uri).groups()\n",
    "    dest_bucket, dest_key = re.match(r\"s3://([^/]+)/(.+)\", destination_uri).groups()\n",
    "    s3.copy_object(Bucket=dest_bucket, CopySource={\"Bucket\": src_bucket, \"Key\": src_key}, Key=dest_key)\n",
    "    s3.delete_object(Bucket=src_bucket, Key=src_key)\n",
    "    print(f\"üì¶ Moved file to archive: {destination_uri}\")\n",
    "\n",
    "def build_read_options(file_format, checkpoint_subpath):\n",
    "    options = {\n",
    "        \"cloudFiles.format\": file_format,\n",
    "        \"cloudFiles.schemaLocation\": checkpoint_subpath,\n",
    "        \"cloudFiles.inferColumnTypes\": \"true\"\n",
    "    }\n",
    "    if file_format == \"csv\":\n",
    "        options.update({\"header\": \"true\", \"delimiter\": \",\"})\n",
    "    elif file_format == \"txt\":\n",
    "        options.update({\"header\": \"true\", \"delimiter\": \"\\t\"})\n",
    "    return options\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN INGESTION FUNCTION\n",
    "# ---------------------------\n",
    "def run_auto_loader_pipeline(config):\n",
    "    print(\"üöÄ Starting Auto Loader Pipeline...\")\n",
    "\n",
    "    catalog = config[\"catalog\"]\n",
    "    schema = config[\"schema\"]\n",
    "    valid_tables = get_valid_tables(catalog, schema)\n",
    "    print(\"‚úÖ Available tables:\", valid_tables)\n",
    "\n",
    "    landing_bucket = re.match(r\"s3://([^/]+)/\", config[\"landing_zone\"]).group(1)\n",
    "    landing_prefix = re.match(r\"s3://[^/]+/(.+)\", config[\"landing_zone\"]).group(1)\n",
    "\n",
    "    files = list(list_s3_files(landing_bucket, landing_prefix))\n",
    "\n",
    "    for file_path in files:\n",
    "        table_name = extract_table_name(file_path, valid_tables)\n",
    "        if not table_name:\n",
    "            print(f\"‚ùå Skipping unrecognized file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        file_format = file_path.split('.')[-1].lower()\n",
    "        print(f\"üì• Loading `{file_path}` ‚Üí `{catalog}.{schema}.{table_name}`\")\n",
    "\n",
    "        read_options = build_read_options(file_format, f\"{config['checkpoint_path']}/{table_name}/schema\")\n",
    "\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"cloudFiles\")\n",
    "            .options(**read_options)\n",
    "            .load(file_path)\n",
    "            .withColumn(\"load_timestamp\", current_timestamp())\n",
    "            .withColumn(\"source_file\", lit(file_path))\n",
    "        )\n",
    "\n",
    "        (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "        )\n",
    "\n",
    "        archive_path = file_path.replace(config[\"landing_zone\"], config[\"archive_zone\"])\n",
    "        move_s3_file(file_path, archive_path)\n",
    "\n",
    "    print(\"‚úÖ Auto Loader Pipeline complete.\")\n",
    "\n",
    "# ---------------------------\n",
    "# EXECUTE PIPELINE\n",
    "# ---------------------------\n",
    "run_auto_loader_pipeline(CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}