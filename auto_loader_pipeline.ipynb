{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Loader Pipeline: Dynamic Schema Ingestion by Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \ud83d\udcc1 CONFIGURATION\n",
    "# =============================================\n",
    "\n",
    "config = {\n",
    "    \"s3_bucket\": \"your-s3-bucket-name\",  # Replace with your actual bucket name\n",
    "    \"landing_prefix\": \"bronze/landing/\",  # Folder in S3 for incoming files\n",
    "    \"archive_prefix\": \"bronze/archive/\",  # Folder for processed file archival\n",
    "    \"bronze_schema\": \"bronze\",  # Target Delta schema in Databricks\n",
    "    \"entity_regex\": r\"edm_(entity[a-zA-Z0-9]*)\",  # Extract entity from file name\n",
    "    \"file_format\": \"auto\",  # Auto-detect format: csv, json, parquet, etc.\n",
    "    \"file_header\": \"true\",  # Consider headers when format is CSV\n",
    "    \"checkpoint_path\": \"dbfs:/mnt/bronze/checkpoints/edm_entity\"  # Checkpoint location\n",
    "}\n",
    "\n",
    "# Derived paths for source and archive\n",
    "config[\"source_path\"] = f\"s3a://{config['s3_bucket']}/{config['landing_prefix']}\"\n",
    "config[\"archive_path\"] = f\"s3a://{config['s3_bucket']}/{config['archive_prefix']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \u2699\ufe0f IMPORTS\n",
    "# =============================================\n",
    "\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \u2699\ufe0f LOAD STREAM FROM S3 USING AUTO LOADER\n",
    "# =============================================\n",
    "\n",
    "def load_stream(config):\n",
    "    \"\"\"Initializes the Auto Loader stream with format auto-detection.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", config[\"file_format\"])\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", config[\"file_header\"])\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "        .option(\"cloudFiles.archiveDir\", config[\"archive_path\"])\n",
    "        .load(config[\"source_path\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \u2699\ufe0f ENRICH DATA WITH ENTITY FROM FILENAME\n",
    "# =============================================\n",
    "\n",
    "def enrich_with_entity(df, config):\n",
    "    \"\"\"Extracts the entity name from file name and adds it as a column.\"\"\"\n",
    "    df = df.withColumn(\"filename\", input_file_name())\n",
    "    df = df.withColumn(\"entity\", regexp_extract(\"filename\", config[\"entity_regex\"], 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \u2699\ufe0f WRITE DATA TO ENTITY-SPECIFIC DELTA TABLES\n",
    "# =============================================\n",
    "\n",
    "def write_entity_tables(bronze_schema):\n",
    "    \"\"\"ForeachBatch logic to dynamically write each entity's data to its own table.\"\"\"\n",
    "    def writer(batch_df, batch_id):\n",
    "        entity_names = batch_df.select(\"entity\").distinct().collect()\n",
    "\n",
    "        for row in entity_names:\n",
    "            entity = row[\"entity\"]\n",
    "            if not entity:\n",
    "                continue\n",
    "\n",
    "            table_name = f\"{bronze_schema}.{entity}\"\n",
    "            entity_df = batch_df.filter(batch_df[\"entity\"] == entity).drop(\"filename\", \"entity\")\n",
    "\n",
    "            try:\n",
    "                if not spark.catalog.tableExists(table_name):\n",
    "                    print(f"\u2705 Creating new table: {table_name}")\n",
    "                    (\n",
    "                        entity_df.write\n",
    "                        .format(\"delta\")\n",
    "                        .mode(\"overwrite\")\n",
    "                        .option(\"overwriteSchema\", \"true\")\n",
    "                        .saveAsTable(table_name)\n",
    "                    )\n",
    "                else:\n",
    "                    print(f"\u2705 Appending to existing table: {table_name}")\n",
    "                    (\n",
    "                        entity_df.write\n",
    "                        .format(\"delta\")\n",
    "                        .mode(\"append\")\n",
    "                        .saveAsTable(table_name)\n",
    "                    )\n",
    "            except AnalysisException as e:\n",
    "                print(f"\u26A0\ufe0f Error writing {table_name}: {str(e)}")\n",
    "\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# \ud83d\ude80 START STREAMING INGESTION PIPELINE\n",
    "# =============================================\n",
    "\n",
    "df_stream = load_stream(config)\n",
    "df_enriched = enrich_with_entity(df_stream, config)\n",
    "\n",
    "query = (\n",
    "    df_enriched.writeStream\n",
    "    .foreachBatch(write_entity_tables(config[\"bronze_schema\"]))\n",
    "    .option(\"checkpointLocation\", config[\"checkpoint_path\"])\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}