# Entity Resolution Auto Loader Pipeline

This notebook implements an automated data ingestion pipeline using Databricks Auto Loader to load data from S3 into Delta tables.

## Architecture

### Database Structure
```
entity_resolution (Database)
└── bronze (Schema)
    ├── edm_entity
    ├── edm_relationship
    └── edm_attribute
```

### S3 Paths
- **Landing Zone**: `s3://bucket/autoloader/laxnding`
  - New files are placed here for processing
  - Organized by entity type (entity, relationship, attribute)
  - Supports CSV, JSON, and Parquet formats

- **Archive Zone**: `s3://bucket/autoloader/archive`
  - Successfully processed files are moved here
  - Maintains same structure as landing zone
  - Includes processing timestamp and status

### Features
- Automatic schema inference and evolution
- Incremental processing with checkpointing
- Error handling and failed file tracking
- Processing metrics and monitoring
- Support for multiple file formats
- Audit trail with source file tracking

# Import required libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
import logging
import json
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AutoLoaderPipeline')

# Pipeline Configuration
class PipelineConfig:
    def __init__(self):
        self.configs = {
            # S3 Paths
            "landing_zone_path": "s3://bucket/autoloader/laxnding",
            "archive_zone_path": "s3://bucket/autoloader/archive",
            
            # Database Configuration
            "database": "entity_resolution",
            "schema": "bronze",
            "catalog_name": "hive_metastore",
            
            # Supported File Formats
            "supported_formats": {
                "csv": {"header": "true", "inferSchema": "true"},
                "json": {"multiLine": "true"},
                "parquet": {}
            },
            
            # Entity Types
            "entity_types": ["edm_entity", "edm_relationship", "edm_attribute"],
            
            # Auto Loader Options
            "auto_loader_options": {
                "cloudFiles.schemaEvolutionMode": "addNewColumns",
                "cloudFiles.inferColumnTypes": "true",
                "cloudFiles.schemaLocation": "/tmp/autoloader/schemas",
                "cloudFiles.useIncrementalListing": "true"
            }
        }
    
    def get_landing_path(self, entity_type):
        """Get the full S3 path for landing zone by entity type"""
        return f"{self.configs['landing_zone_path']}/{entity_type}"
    
    def get_archive_path(self, entity_type, status="processed"):
        """Get the full S3 path for archive zone by entity type and status"""
        timestamp = datetime.now().strftime('%Y/%m/%d')
        return f"{self.configs['archive_zone_path']}/{entity_type}/{status}/{timestamp}"
    
    def get_table_name(self, entity_type):
        """Get the full table name including database and schema"""
        return f"{self.configs['database']}.{self.configs['schema']}.{entity_type}"

# Initialize configuration
config = PipelineConfig()

# Create Spark session with optimized configs
spark = SparkSession.builder \
    .appName("Entity-Resolution-AutoLoader") \
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/autoloader/checkpoints") \
    .config("spark.sql.streaming.metricsEnabled", "true") \
    .getOrCreate()

# Log configuration
logger.info("Configuration loaded successfully!")
logger.info(f"Landing zone path: {config.configs['landing_zone_path']}")
logger.info(f"Archive zone path: {config.configs['archive_zone_path']}")
logger.info(f"Database: {config.configs['database']}")
logger.info(f"Schema: {config.configs['schema']}")
logger.info(f"Supported formats: {list(config.configs['supported_formats'].keys())}")
logger.info(f"Entity types: {config.configs['entity_types']}")

class AutoLoaderPipeline:
    def __init__(self, config):
        self.config = config
        self.metrics = {
            "processed_files": 0,
            "failed_files": 0,
            "processed_records": 0,
            "start_time": None,
            "end_time": None,
            "errors": []
        }
    
    def setup_database(self):
        """Create database and schema if they don't exist"""
        try:
            # Create database
            spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.config.configs['database']}")
            logger.info(f"Database {self.config.configs['database']} created/verified")
            
            # Create schema
            spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.config.configs['database']}.{self.config.configs['schema']}")
            logger.info(f"Schema {self.config.configs['database']}.{self.config.configs['schema']} created/verified")
            
            return True
        except Exception as e:
            logger.error(f"Error setting up database: {str(e)}")
            self.metrics["errors"].append({"type": "database_setup", "error": str(e)})
            return False
    
    def detect_file_format(self, path):
        """Detect the file format of files in the path"""
        try:
            files = dbutils.fs.ls(path)
            if not files:
                return None
                
            # Get the first file's extension
            first_file = [f for f in files if not f.path.endswith('/')][0]
            extension = first_file.path.split('.')[-1].lower()
            
            format_mapping = {
                'csv': 'csv',
                'json': 'json',
                'parquet': 'parquet',
                'txt': 'csv'  # Treat .txt as CSV
            }
            
            return format_mapping.get(extension, 'csv')  # Default to CSV if unknown
        except Exception as e:
            logger.warning(f"Error detecting file format: {str(e)}")
            return 'csv'  # Default to CSV on error
    
    def process_entity_type(self, entity_type):
        """Process files for a specific entity type using Auto Loader"""
        try:
            # Get paths
            source_path = self.config.get_landing_path(entity_type)
            checkpoint_path = f"/tmp/autoloader/checkpoints/{entity_type}"
            
            # Detect file format
            file_format = self.detect_file_format(source_path)
            if not file_format:
                logger.info(f"No files found for {entity_type}")
                return True
                
            logger.info(f"Processing {entity_type} using format: {file_format}")
            
            # Set up Auto Loader options
            options = {
                **self.config.configs['auto_loader_options'],
                **self.config.configs['supported_formats'][file_format],
                "cloudFiles.format": file_format,
                "cloudFiles.schemaLocation": f"{checkpoint_path}/schema"
            }
            
            # Read stream
            df = (spark.readStream
                .format("cloudFiles")
                .options(**options)
                .load(source_path))
            
            # Add metadata columns
            df = (df.withColumn("source_file", input_file_name())
                .withColumn("processed_at", current_timestamp())
                .withColumn("year", year(current_timestamp()))
                .withColumn("month", month(current_timestamp()))
                .withColumn("day", dayofmonth(current_timestamp())))
            
            # Write stream to Delta table
            table_name = self.config.get_table_name(entity_type)
            
            query = (df.writeStream
                .format("delta")
                .option("checkpointLocation", f"{checkpoint_path}/write")
                .option("mergeSchema", "true")
                .partitionBy("year", "month", "day")
                .trigger(once=True)
                .toTable(table_name))
            
            # Wait for the query to complete
            query.awaitTermination()
            
            # Archive processed files
            self._archive_processed_files(entity_type, source_path)
            
            # Update metrics
            self.metrics["processed_files"] += 1
            
            logger.info(f"Successfully processed {entity_type}")
            return True
            
        except Exception as e:
            logger.error(f"Error processing {entity_type}: {str(e)}")
            self.metrics["errors"].append({
                "type": "processing",
                "entity_type": entity_type,
                "error": str(e)
            })
            self.metrics["failed_files"] += 1
            return False
    
    def _archive_processed_files(self, entity_type, source_path):
        """Archive processed files"""
        try:
            archive_path = self.config.get_archive_path(entity_type)
            files = dbutils.fs.ls(source_path)
            
            for file_info in files:
                if not file_info.path.endswith('/'):  # Skip directories
                    file_name = file_info.path.split('/')[-1]
                    target_path = f"{archive_path}/{file_name}"
                    dbutils.fs.mv(file_info.path, target_path)
                    logger.info(f"Archived {file_name} to {target_path}")
            
        except Exception as e:
            logger.error(f"Error archiving files for {entity_type}: {str(e)}")
            self.metrics["errors"].append({
                "type": "archiving",
                "entity_type": entity_type,
                "error": str(e)
            })
    
    def run(self):
        """Run the Auto Loader pipeline"""
        self.metrics["start_time"] = datetime.now()
        logger.info("Starting Auto Loader pipeline")
        
        # Setup database and schema
        if not self.setup_database():
            logger.error("Failed to set up database and schema")
            return self.metrics
        
        # Process each entity type
        for entity_type in self.config.configs["entity_types"]:
            self.process_entity_type(entity_type)
        
        self.metrics["end_time"] = datetime.now()
        duration = (self.metrics["end_time"] - self.metrics["start_time"]).total_seconds()
        logger.info(f"Pipeline completed in {duration:.2f} seconds")
        
        return self.metrics

# Initialize and run the pipeline
pipeline = AutoLoaderPipeline(config)
metrics = pipeline.run()

# Display metrics
print("\n=== Pipeline Metrics ===")
print(f"Start Time: {metrics['start_time']}")
print(f"End Time: {metrics['end_time']}")
print(f"Files Processed: {metrics['processed_files']}")
print(f"Files Failed: {metrics['failed_files']}")
if metrics['errors']:
    print("\nErrors:")
    for error in metrics['errors']:
        print(f"- {error['type']}: {error['error']}")

# Display table information
def display_table_info():
    """Display information about processed tables"""
    for entity_type in config.configs["entity_types"]:
        table_name = config.get_table_name(entity_type)
        try:
            # Get table statistics
            df = spark.table(table_name)
            row_count = df.count()
            
            print(f"\n=== {entity_type} ===")
            print(f"Table: {table_name}")
            print(f"Row count: {row_count:,}")
            
            # Show schema
            print("\nSchema:")
            df.printSchema()
            
            # Show sample data
            print("\nSample Data:")
            df.limit(5).show(truncate=False)
            
        except Exception as e:
            print(f"Error getting info for {table_name}: {str(e)}")

# Display table information
display_table_info()

## How to Use This Notebook

1. **Prerequisites**
   - Access to S3 bucket with read/write permissions
   - Databricks runtime with Auto Loader support
   - Proper mount points or IAM roles configured

2. **S3 Structure**
   ```
   s3://bucket/autoloader/
   ├── laxnding/                    # Landing zone
   │   ├── edm_entity/             # Place entity files here
   │   ├── edm_relationship/       # Place relationship files here
   │   └── edm_attribute/         # Place attribute files here
   └── archive/                    # Processed files moved here automatically
   ```

3. **Supported File Formats**
   - CSV (default, including .txt files)
   - JSON
   - Parquet
   - Files should include headers for CSV/TXT

4. **Tables Created**
   - `entity_resolution.bronze.edm_entity`
   - `entity_resolution.bronze.edm_relationship`
   - `entity_resolution.bronze.edm_attribute`

5. **Features**
   - Automatic schema inference and evolution
   - File format auto-detection
   - Incremental processing with checkpointing
   - Error handling and failed file tracking
   - Detailed logging and metrics
   - Partitioned tables by year/month/day
   - Source file tracking and processing timestamp
   - Automatic file archiving

6. **Monitoring**
   - Check notebook output for processing status
   - Review table statistics after processing
   - Monitor archive location for processed files
   - Check error logs for any issues

7. **Best Practices**
   - Place files in the correct entity type folder
   - Use consistent file formats per entity type
   - Monitor checkpoint location usage
   - Review archived files periodically
   - Check processing metrics after each run

# Function to process files using Auto Loader
def process_files(table_name):
    """Process files from S3 landing zone for a specific table using Auto Loader."""
    try:
        # Source path in S3
        source_path = f"{configs['landing_zone_path']}/{table_name}"
        
        # Checkpoint location for Auto Loader
        checkpoint_path = f"/tmp/checkpoints/{table_name}"
        
        # Archive path
        archive_path = f"{configs['archive_zone_path']}/{table_name}/processed"
        
        print(f"📖 Processing files for table: {table_name}")
        print(f"Source path: {source_path}")
        print(f"Checkpoint path: {checkpoint_path}")
        print(f"Archive path: {archive_path}")
        
        # Read stream using Auto Loader
        df = (spark.readStream.format("cloudFiles")
            .option("cloudFiles.format", "csv")  # Change based on your file format
            .option("cloudFiles.schemaLocation", checkpoint_path)
            .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
            .option("cloudFiles.inferColumnTypes", "true")
            .option("header", "true")
            .load(source_path))
            
        # Add metadata columns
        df = (df.withColumn("source_file", input_file_name())
            .withColumn("processed_at", current_timestamp()))
            
        # Write to Delta table
        query = (df.writeStream
            .format("delta")
            .option("checkpointLocation", f"{checkpoint_path}/write")
            .option("mergeSchema", "true")
            .trigger(once=True)  # Process files once and stop
            .toTable(f"{configs['database']}.{configs['schema']}.{table_name}"))
            
        # Wait for the query to complete
        query.awaitTermination()
        
        # Move processed files to archive
        dbutils.fs.mv(source_path, archive_path, recurse=True)
        
        return True
        
    except Exception as e:
        print(f"❌ Error processing {table_name}: {str(e)}")
        return False

print("✅ Auto Loader function defined successfully!")

# Main execution
print("🚀 Starting Auto Loader Pipeline...\n")

# Create database and schema
print(f"Creating database: {configs['database']}")
spark.sql(f"CREATE DATABASE IF NOT EXISTS {configs['database']}")

print(f"\nCreating schema: {configs['database']}.{configs['schema']}")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {configs['database']}.{configs['schema']}")

# Show available databases and schemas
print("\nAvailable databases:")
spark.sql("SHOW DATABASES").show()

print("\nAvailable schemas in database:")
spark.sql(f"SHOW SCHEMAS IN {configs['database']}").show()

# Process each table type
tables = ['edm_entity', 'edm_relationship', 'edm_attribute']
results = {'processed': [], 'failed': []}

for table in tables:
    print(f"\n📝 Processing table: {table}")
    if process_files(table):
        results['processed'].append(table)
    else:
        results['failed'].append(table)

# Print summary
print("\n=== Processing Summary ===")
print(f"✅ Successfully processed tables: {', '.join(results['processed'])}")
if results['failed']:
    print(f"❌ Failed tables: {', '.join(results['failed'])}")
else:
    print("❌ No failed tables")

print("\n✨ Pipeline completed!")

## How to Use This Notebook

1. **S3 Configuration**
   - The notebook is configured to use these S3 paths:
     ```python
     "landing_zone_path": "s3://bucket/autoloader/laxnding"
     "archive_zone_path": "s3://bucket/autoloader/archive"
     ```

2. **S3 Structure**
   ```
   autoloader/
   ├── laxnding/               # Place new files here
   │   ├── edm_entity/         # Entity files
   │   ├── edm_relationship/   # Relationship files
   │   └── edm_attribute/      # Attribute files
   └── archive/                # Processed files moved here
   ```

3. **File Format Support**
   - Default: CSV files with headers
   - To support other formats (JSON, Parquet), modify the `cloudFiles.format` option in the `process_files` function

4. **Tables Created**
   - `entity_resolution.bronze.edm_entity`
   - `entity_resolution.bronze.edm_relationship`
   - `entity_resolution.bronze.edm_attribute`

5. **Features**
   - Automatic schema evolution
   - File archiving after processing
   - Processing metadata columns (source_file, processed_at)
   - Checkpoint-based processing to avoid duplicates

6. **Monitoring**
   - Check processing status in the notebook output
   - Verify processed files in the archive location
   - Query tables to validate data loading

# Function to process files using Auto Loader
def process_files(table_name):
    """Process files from S3 landing zone for a specific table using Auto Loader."""
    try:
        # Source path in S3
        source_path = f"{configs['landing_zone_path']}/{table_name}"
        
        # Checkpoint location for Auto Loader
        checkpoint_path = f"/tmp/checkpoints/{table_name}"
        
        # Archive path
        archive_path = f"{configs['archive_zone_path']}/{table_name}/processed"
        
        print(f"📖 Processing files for table: {table_name}")
        print(f"Source path: {source_path}")
        print(f"Checkpoint path: {checkpoint_path}")
        print(f"Archive path: {archive_path}")
        
        # Read stream using Auto Loader
        df = spark.readStream.format("cloudFiles") \
            .option("cloudFiles.format", "csv") \  # Change based on your file format
            .option("cloudFiles.schemaLocation", checkpoint_path) \
            .option("cloudFiles.schemaEvolutionMode", "addNewColumns") \
            .option("cloudFiles.inferColumnTypes", "true") \
            .option("header", "true") \
            .load(source_path)
            
        # Add metadata columns
        df = df.withColumn("source_file", input_file_name()) \
            .withColumn("processed_at", current_timestamp())
            
        # Write to Delta table
        query = df.writeStream \
            .format("delta") \
            .option("checkpointLocation", f"{checkpoint_path}/write") \
            .option("mergeSchema", "true") \
            .trigger(once=True) \  # Process files once and stop
            .toTable(f"{configs['database']}.{configs['schema']}.{table_name}")
            
        # Wait for the query to complete
        query.awaitTermination()
        
        # Move processed files to archive
        dbutils.fs.mv(source_path, archive_path, recurse=True)
        
        return True
        
    except Exception as e:
        print(f"❌ Error processing {table_name}: {str(e)}")
        return False

print("✅ Auto Loader function defined successfully!")
