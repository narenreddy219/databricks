# Import required libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
import logging
import json
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('AutoLoaderPipeline')

# Pipeline Configuration
class PipelineConfig:
    def __init__(self):
        self.configs = {
            # S3 Paths
            "landing_zone_path": "s3://bucket/autoloader/laxnding",
            "archive_zone_path": "s3://bucket/autoloader/archive",
            
            # Database Configuration
            "database": "entity_resolution",
            "schema": "bronze",
            "catalog_name": "hive_metastore",
            
            # Supported File Formats
            "supported_formats": {
                "csv": {"header": "true", "inferSchema": "true"},
                "json": {"multiLine": "true"},
                "parquet": {}
            },
            
            # Entity Types
            "entity_types": ["edm_entity", "edm_relationship", "edm_attribute"],
            
            # Auto Loader Options
            "auto_loader_options": {
                "cloudFiles.schemaEvolutionMode": "addNewColumns",
                "cloudFiles.inferColumnTypes": "true",
                "cloudFiles.schemaLocation": "/tmp/autoloader/schemas",
                "cloudFiles.useIncrementalListing": "true"
            }
        }
    
    def get_landing_path(self, entity_type):
        """Get the full S3 path for landing zone by entity type"""
        return f"{self.configs['landing_zone_path']}/{entity_type}"
    
    def get_archive_path(self, entity_type, status="processed"):
        """Get the full S3 path for archive zone by entity type and status"""
        timestamp = datetime.now().strftime('%Y/%m/%d')
        return f"{self.configs['archive_zone_path']}/{entity_type}/{status}/{timestamp}"
    
    def get_table_name(self, entity_type):
        """Get the full table name including database and schema"""
        return f"{self.configs['database']}.{self.configs['schema']}.{entity_type}"

# Initialize configuration
config = PipelineConfig()

# Create Spark session with optimized configs
spark = SparkSession.builder \
    .appName("Entity-Resolution-AutoLoader") \
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/autoloader/checkpoints") \
    .config("spark.sql.streaming.metricsEnabled", "true") \
    .getOrCreate()

# Log configuration
logger.info("Configuration loaded successfully!")
logger.info(f"Landing zone path: {config.configs['landing_zone_path']}")
logger.info(f"Archive zone path: {config.configs['archive_zone_path']}")
logger.info(f"Database: {config.configs['database']}")
logger.info(f"Schema: {config.configs['schema']}")
logger.info(f"Supported formats: {list(config.configs['supported_formats'].keys())}")
logger.info(f"Entity types: {config.configs['entity_types']}")
