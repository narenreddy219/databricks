{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a1991a",
   "metadata": {},
   "source": [
    "# üì¶ Simple Databricks Auto Loader Pipeline\n",
    "This notebook demonstrates a simple Auto Loader pipeline that:\n",
    "- Loads data from S3 (any supported format)\n",
    "- Extracts the target table name from the file name (e.g., `edm_entity_2024-06-01.csv` ‚Üí `edm_entity`)\n",
    "- Performs checkpointing and schema inference\n",
    "- Adds metadata columns like source file and load timestamp\n",
    "- Moves processed files to an S3 archive zone\n",
    "- Only writes to tables that exist in the Bronze schema of the `entity_resolution_dev` catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efff4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp, lit\n",
    "import re\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------\n",
    "landing_zone     = 's3://your-bucket/landing-zone/'\n",
    "archive_zone     = 's3://your-bucket/archive-zone/'\n",
    "catalog          = 'entity_resolution_dev'\n",
    "schema           = 'bronze'\n",
    "checkpoint_path  = '/tmp/autoloader/checkpoints'\n",
    "\n",
    "# ----------------------------------------\n",
    "# GET LIST OF EXISTING BRONZE TABLES\n",
    "# ----------------------------------------\n",
    "tables = [t.name for t in spark.catalog.listTables(f'{catalog}.{schema}')]\n",
    "print('‚úÖ Available Bronze tables:', tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------\n",
    "def extract_table_name(file_path):\n",
    "    \"\"\"\n",
    "    Extract table name from filename. \n",
    "    e.g., s3://.../edm_entity_2024-06-01.csv ‚Üí edm_entity\n",
    "    \"\"\"\n",
    "    match = re.search(r'/([a-zA-Z0-9_]+)[^/]*\\.', file_path)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def detect_format(file_path):\n",
    "    \"\"\"\n",
    "    Detect file format from extension\n",
    "    \"\"\"\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    return ext if ext in ['csv', 'json', 'parquet', 'txt'] else 'csv'\n",
    "\n",
    "print('‚úÖ Configuration complete. Ready to process incoming files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# MAIN: PROCESS EACH NEW FILE IN LANDING ZONE\n",
    "# ----------------------------------------\n",
    "files = dbutils.fs.ls(landing_zone)\n",
    "\n",
    "for file in files:\n",
    "    file_path = file.path\n",
    "    table_name = extract_table_name(file_path)\n",
    "\n",
    "    if not table_name:\n",
    "        print(f\"‚ö†Ô∏è Could not extract table name from {file_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if table_name not in tables:\n",
    "        print(f\"‚ö†Ô∏è Table {table_name} not found in {catalog}.{schema}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    file_format = detect_format(file_path)\n",
    "    print(f\"üîÑ Processing: {file_path} as {file_format} into {catalog}.{schema}.{table_name}\")\n",
    "\n",
    "    # Auto Loader options\n",
    "    options = {\n",
    "        'cloudFiles.format': file_format,\n",
    "        'cloudFiles.schemaLocation': f'{checkpoint_path}/{table_name}/schema',\n",
    "        'cloudFiles.inferColumnTypes': 'true'\n",
    "    }\n",
    "\n",
    "    # File-type-specific tweaks\n",
    "    if file_format == 'csv':\n",
    "        options['header'] = 'true'\n",
    "        options['delimiter'] = ','\n",
    "    elif file_format == 'txt':\n",
    "        options['header'] = 'true'\n",
    "        options['delimiter'] = '\\t'\n",
    "\n",
    "    # Read file with Auto Loader\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format('cloudFiles')\n",
    "        .options(**options)\n",
    "        .load(file_path)\n",
    "        .withColumn('source_file', lit(file_path))\n",
    "        .withColumn('load_timestamp', current_timestamp())\n",
    "    )\n",
    "\n",
    "    # Write to Bronze Delta Table\n",
    "    (\n",
    "        df.write\n",
    "        .format('delta')\n",
    "        .mode('append')\n",
    "        .option('mergeSchema', 'true')\n",
    "        .saveAsTable(f'{catalog}.{schema}.{table_name}')\n",
    "    )\n",
    "\n",
    "    # Archive the file\n",
    "    archive_path = archive_zone + file_path.replace(landing_zone, '')\n",
    "    dbutils.fs.mv(file_path, archive_path)\n",
    "    print(f\"‚úÖ Archived: {file_path} ‚Üí {archive_path}\")\n",
    "\n",
    "print('üéâ Auto Loader pipeline execution complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}