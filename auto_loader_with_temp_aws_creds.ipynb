{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88bcc7b",
   "metadata": {},
   "source": [
    "# 🚀 Databricks Auto Loader Pipeline with Temporary AWS Credentials and S3 Archiving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92b11c",
   "metadata": {},
   "source": [
    "\n",
    "This notebook securely retrieves temporary AWS credentials from a custom identity service,\n",
    "injects them into Spark for Auto Loader access, streams JSON files from S3 to a Bronze Delta table,\n",
    "and archives the processed files using `boto3`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44649c",
   "metadata": {},
   "source": [
    "## 🔐 Step 1: Retrieve AWS Credentials Securely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=10), stop=stop_after_attempt(3))\n",
    "def get_aws_token_info(\n",
    "    aws_username: str,\n",
    "    aws_account_id: str,\n",
    "    aws_role_name: str,\n",
    "    aws_password: str,\n",
    "    ca_bundle_file: str\n",
    ") -> dict:\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=\"https://epi.v4ic-identity.ssov.factset.com/creds\",\n",
    "            json={\n",
    "                \"username\": aws_username,\n",
    "                \"accountId\": aws_account_id,\n",
    "                \"roleName\": aws_role_name,\n",
    "                \"password\": aws_password\n",
    "            },\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            verify=ca_bundle_file,\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to retrieve AWS credentials: {response.text}\")\n",
    "        return response.json()\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def get_client(service, aws_username, aws_account_id, aws_role_name, aws_password, ca_bundle_file, region_name=\"us-east-1\") -> Tuple[boto3.client, datetime]:\n",
    "    creds = get_aws_token_info(aws_username, aws_account_id, aws_role_name, aws_password, ca_bundle_file)\n",
    "    expire_time = datetime.strptime(creds[\"awsExpires\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    client = boto3.client(\n",
    "        service,\n",
    "        aws_access_key_id=creds[\"awsAccessKey\"],\n",
    "        aws_secret_access_key=creds[\"awsSecretKey\"],\n",
    "        aws_session_token=creds[\"awsSessionToken\"],\n",
    "        region_name=region_name\n",
    "    )\n",
    "    return client, expire_time\n",
    "\n",
    "def get_s3_client(aws_username, aws_account_id, aws_role_name, aws_password, ca_bundle_file):\n",
    "    return get_client(\"s3\", aws_username, aws_account_id, aws_role_name, aws_password, ca_bundle_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739e9df",
   "metadata": {},
   "source": [
    "## 🔧 Step 2: Configure Secrets and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with Databricks secrets or manual values for testing\n",
    "AWS_USERNAME = dbutils.secrets.get(\"my_scope\", \"aws_username\")\n",
    "AWS_PASSWORD = dbutils.secrets.get(\"my_scope\", \"aws_password\")\n",
    "AWS_ACCOUNT_ID = dbutils.secrets.get(\"my_scope\", \"aws_account_id\")\n",
    "AWS_ROLE_NAME = dbutils.secrets.get(\"my_scope\", \"aws_role_name\")\n",
    "CA_BUNDLE_FILE = dbutils.secrets.get(\"my_scope\", \"ca_bundle_file\")\n",
    "\n",
    "# Get S3 client and expiry time\n",
    "s3_client, s3_expiration_time = get_s3_client(\n",
    "    aws_username=AWS_USERNAME,\n",
    "    aws_account_id=AWS_ACCOUNT_ID,\n",
    "    aws_role_name=AWS_ROLE_NAME,\n",
    "    aws_password=AWS_PASSWORD,\n",
    "    ca_bundle_file=CA_BUNDLE_FILE\n",
    ")\n",
    "\n",
    "# Inject credentials into Spark session\n",
    "def inject_aws_creds_into_spark(creds):\n",
    "    spark.conf.set(\"fs.s3a.access.key\", creds[\"awsAccessKey\"])\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", creds[\"awsSecretKey\"])\n",
    "    spark.conf.set(\"fs.s3a.session.token\", creds[\"awsSessionToken\"])\n",
    "\n",
    "aws_creds = get_aws_token_info(AWS_USERNAME, AWS_ACCOUNT_ID, AWS_ROLE_NAME, AWS_PASSWORD, CA_BUNDLE_FILE)\n",
    "inject_aws_creds_into_spark(aws_creds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35ed9f",
   "metadata": {},
   "source": [
    "## 📥 Step 3: Read Stream from S3 Using Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2fad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "bucket_name = \"fdss3-entity-resolution-data-prod\"\n",
    "subdirectory = \"input/\"\n",
    "source_path = f\"s3a://{bucket_name}/{subdirectory}\"\n",
    "\n",
    "schema = StructType().add(\"id\", StringType()).add(\"value\", StringType())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(source_path)\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab6507",
   "metadata": {},
   "source": [
    "## 💾 Step 4: Write to Bronze Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bronze_table_path = \"/mnt/bronze/entity_data\"\n",
    "checkpoint_path = \"/mnt/bronze/checkpoints/entity_data\"\n",
    "\n",
    "(\n",
    "    df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start(bronze_table_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06231d",
   "metadata": {},
   "source": [
    "## 📦 Step 5: Archive Processed Files Using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af960f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def archive_files(s3_client, bucket_name, source_prefix, archive_prefix):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=source_prefix)\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        filename = os.path.basename(key)\n",
    "        if not filename:\n",
    "            continue\n",
    "        file_prefix = filename.split(\"_\")[0]\n",
    "        dest_key = f\"{archive_prefix}{file_prefix}/{filename}\"\n",
    "        s3_client.copy_object(Bucket=bucket_name, CopySource={\"Bucket\": bucket_name, \"Key\": key}, Key=dest_key)\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "        print(f\"Archived: {key} → {dest_key}\")\n",
    "\n",
    "# Example usage (manual trigger after stream)\n",
    "archive_files(s3_client, bucket_name, \"input/\", \"archive/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14425d8",
   "metadata": {},
   "source": [
    "## ✅ Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5673d",
   "metadata": {},
   "source": [
    "\n",
    "- Temporary AWS credentials are fetched securely from a custom identity service\n",
    "- Credentials are injected into Spark for Auto Loader to access S3\n",
    "- Data is streamed into a Bronze Delta table using `cloudFiles`\n",
    "- Processed files are archived to structured S3 folders using `boto3`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
