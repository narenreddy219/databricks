{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497ca3ca",
   "metadata": {},
   "source": [
    "# Delta Streaming Pipeline: Refactored and Py4J-Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d73482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"DeltaIngest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc65d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StreamConfig:\n",
    "    \"\"\"Configuration for stream ingestion.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.catalog = \"hive_metastore\"\n",
    "        self.schema = \"bronze\"\n",
    "        self.table_name = \"entity_table\"\n",
    "        self.source_path = \"s3a://your-bucket/entity-data/\"\n",
    "        self.checkpoint_path = \"s3a://your-bucket/checkpoints/entity\"\n",
    "        self.merge_keys = [\"entity_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a38237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_spark() -> SparkSession:\n",
    "    \"\"\"Initializes and returns a SparkSession configured for Delta Lake.\"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"DeltaLakeIngest\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize SparkSession: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08451ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataValidation:\n",
    "    \"\"\"Handles validation logic for input DataFrames.\"\"\"\n",
    "\n",
    "    def __init__(self, merge_keys: List[str]):\n",
    "        self.merge_keys = merge_keys\n",
    "\n",
    "    def validate(self, df: DataFrame) -> bool:\n",
    "        \"\"\"Validates DataFrame to ensure merge keys are not null or empty.\"\"\"\n",
    "        try:\n",
    "            for key in self.merge_keys:\n",
    "                null_count = df.filter(df[key].isNull() | (df[key] == '')).count()\n",
    "                if null_count > 0:\n",
    "                    logger.warning(f\"Validation failed: Primary key '{key}' has {null_count} null/empty values\")\n",
    "                    return False\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data validation error: {str(e)}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_to_delta(spark: SparkSession, df: DataFrame, config: StreamConfig):\n",
    "    \"\"\"Merges the input DataFrame into the target Delta table.\"\"\"\n",
    "    try:\n",
    "        full_table_name = f\"{config.catalog}.{config.schema}.{config.table_name}\"\n",
    "\n",
    "        if not DeltaTable.isDeltaTable(spark, f\"{config.catalog}.{config.schema}.{config.table_name}\"):\n",
    "            logger.info(f\"Creating new Delta table: {full_table_name}\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "            return\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, full_table_name)\n",
    "        merge_condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in config.merge_keys])\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            source=df.alias(\"source\"),\n",
    "            condition=merge_condition\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        logger.info(f\"Merged data into Delta table: {full_table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Delta merge: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43208c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_stream(spark: SparkSession, config: StreamConfig):\n",
    "    \"\"\"Starts the Auto Loader stream for ingesting data.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Auto Loader stream...\")\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"entity_id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"event_time\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        raw_df = spark.readStream \\\n",
    "            .format(\"cloudFiles\") \\\n",
    "            .option(\"cloudFiles.format\", \"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(schema) \\\n",
    "            .load(config.source_path)\n",
    "\n",
    "        df_with_ts = raw_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "        def foreach_batch_function(batch_df, batch_id):\n",
    "            logger.info(f\"Processing batch {batch_id} ...\")\n",
    "            validator = DataValidation(config.merge_keys)\n",
    "            if validator.validate(batch_df):\n",
    "                merge_to_delta(spark, batch_df, config)\n",
    "            else:\n",
    "                logger.warning(f\"Skipping batch {batch_id} due to validation failure\")\n",
    "\n",
    "        query = df_with_ts.writeStream \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"checkpointLocation\", config.checkpoint_path) \\\n",
    "            .foreachBatch(foreach_batch_function) \\\n",
    "            .start()\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in stream processing: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = StreamConfig()\n",
    "    spark = setup_spark()\n",
    "    start_stream(spark, config)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
