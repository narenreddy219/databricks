# autoloader_process_batch.py

import os
from pyspark.sql.functions import input_file_name

# === Cell 1: Configuration ===
# Define reusable configuration variables for Auto Loader
org_name = "entity"  # replace with your actual org name
schema_name = "edm"   # logical schema for the bronze layer
target_bucket = "your-bucket"  # root S3 bucket
landing_path = f"s3://{target_bucket}/landing-zone/"
router_schema_path = f"s3://{target_bucket}/schema-tracking/__router__/"
router_checkpoint_path = f"s3://{target_bucket}/checkpoints/__router__/"

# === Cell 2: File Detection with Auto Loader ===
# Detect all files arriving in the landing zone
raw_df = (
    spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "binaryFile")  # detect filenames only
    .option("cloudFiles.includeExistingFiles", "true")
    .option("cloudFiles.schemaLocation", router_schema_path)
    .load(landing_path)
    .withColumn("source_file", input_file_name())
)

def process_batch(batch_df, batch_id, landing_path, org_name, schema_name):
    """
    Processes files in a micro-batch, routes by filename, loads into Delta tables,
    and archives the file after successful ingestion.

    Args:
        batch_df: DataFrame from Auto Loader (contains 'path' column)
        batch_id: Streaming batch ID
        landing_path: Base S3 path for the landing zone (e.g., s3://your-bucket/landing-zone/)
        org_name: Organization prefix used in table names (e.g., 'entity')
        schema_name: Schema name used in Delta Lake structure (e.g., 'edm')
    """

    # === Validation: Check required arguments ===
    if not landing_path or not landing_path.startswith("s3://"):
        raise ValueError("landing_path must be a valid S3 path (e.g., s3://bucket/path)")

    if not org_name:
        raise ValueError("org_name must be provided")

    if not schema_name:
        raise ValueError("schema_name must be provided")

    if batch_df is None or batch_df.isEmpty():
        print("‚ö†Ô∏è Empty batch received. Skipping processing.")
        return

    # Extract bucket name from landing path
    bucket = landing_path.replace("s3://", "").split("/")[0]

    paths = batch_df.select("path").rdd.map(lambda r: r[0]).collect()

    for path in paths:
        try:
            filename = os.path.basename(path)
            ext = os.path.splitext(filename)[1].lower().strip(".")
            filename_lower = filename.lower()

            # === Validation: Ensure file path and extension ===
            if not filename:
                print(f"‚ùå Skipping unnamed file at path: {path}")
                continue

            if not ext:
                print(f"‚ùå Could not determine file extension: {filename}")
                continue

            # Determine routing
            if "edm_entity" in filename_lower:
                table_name = "edm_entity"
            else:
                print(f"‚ùå No routing rule for file: {filename}")
                continue

            # Build dynamic paths from config
            target_table = f"{org_name}.bronze.{table_name}"
            schema_tracking_path = f"s3://{bucket}/schema-tracking/{org_name}/{schema_name}/{table_name}/"
            checkpoint_path = f"s3://{bucket}/checkpoints/{org_name}/{schema_name}/{table_name}/"

            print(f"üîç Routing file: {filename} ‚Üí Table: {target_table}")

            # Read file based on format
            if ext in ["csv", "txt"]:
                df = spark.read.option("header", "true").option("inferSchema", "true").csv(path)
            elif ext == "json":
                df = spark.read.option("inferSchema", "true").json(path)
            else:
                print(f"‚ö†Ô∏è Unsupported file format: {filename}")
                continue

            # === Validation: Ensure DataFrame is not empty ===
            if df.rdd.isEmpty():
                print(f"‚ö†Ô∏è Skipping empty file: {filename}")
                continue

            df = df.withColumn("source_file", input_file_name())

            # Validate table existence
            if not spark._jsparkSession.catalog().tableExists(target_table):
                print(f"‚ùå Table not found: {target_table}. File left in landing zone.")
                continue

            # Write to Delta table
            df.write.format("delta").mode("append").saveAsTable(target_table)
            print(f"‚úÖ Data appended to: {target_table}")

            # Archive processed file
            archive_path = path.replace("landing-zone", "archive-zone")
            dbutils.fs.mv(path, archive_path)
            print(f"üì¶ Archived: {archive_path}")

        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")
