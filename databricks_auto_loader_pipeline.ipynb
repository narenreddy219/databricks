{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql.functions import input_file_name\n", "import re\n", "import boto3\n", "from urllib.parse import urlparse\n", "\n", "# -------------------------\n", "# Configurations\n", "# -------------------------\n", "source_path = \"s3://your-bucket/raw/\"\n", "archive_path = \"s3://your-bucket/archive/\"\n", "checkpoint_path = \"s3://your-bucket/checkpoints/\"\n", "bronze_db = \"bronze\"\n", "\n", "# -------------------------\n", "# Extract clean table name from file path\n", "# edm_entity1.txt \u2192 edm_entity\n", "# -------------------------\n", "def extract_table_name(file_path):\n", "    match = re.search(r'/([^/]+)\\.txt$', file_path)\n", "    if match:\n", "        filename = match.group(1)\n", "        table_name = re.sub(r'\\d+$', '', filename)  # Remove trailing digits\n", "        return table_name\n", "    return \"unknown_table\"\n", "\n", "# -------------------------\n", "# Move file from raw \u2192 archive\n", "# -------------------------\n", "def move_to_archive_s3(source_url, archive_url):\n", "    s3 = boto3.client(\"s3\")\n", "    source = urlparse(source_url)\n", "    archive = urlparse(archive_url)\n", "\n", "    s3.copy_object(\n", "        Bucket=archive.netloc,\n", "        CopySource={'Bucket': source.netloc, 'Key': source.path.lstrip(\"/\")},\n", "        Key=archive.path.lstrip(\"/\")\n", "    )\n", "    s3.delete_object(Bucket=source.netloc, Key=source.path.lstrip(\"/\"))\n", "\n", "# -------------------------\n", "# Step 1: Read with Auto Loader\n", "# -------------------------\n", "raw_df = (\n", "    spark.readStream.format(\"cloudFiles\")\n", "    .option(\"cloudFiles.format\", \"text\")\n", "    .option(\"cloudFiles.schemaLocation\", checkpoint_path + \"schema/\")\n", "    .load(source_path)\n", ")\n", "\n", "df_with_file = raw_df.withColumn(\"input_file\", input_file_name())\n", "\n", "# -------------------------\n", "# Step 2: Route to table and archive\n", "# -------------------------\n", "def process_and_archive(batch_df, batch_id):\n", "    if batch_df.isEmpty():\n", "        return\n", "\n", "    file_path = batch_df.select(\"input_file\").first()[\"input_file\"]\n", "    base_table = extract_table_name(file_path)\n", "    full_table = f\"{bronze_db}.{base_table}\"\n", "    bronze_table_path = f\"/mnt/bronze/{base_table}\"\n", "\n", "    batch_df.drop(\"input_file\").write.format(\"delta\") \\\n", "        .mode(\"append\") \\\n", "        .option(\"mergeSchema\", \"true\") \\\n", "        .save(bronze_table_path)\n", "\n", "    spark.sql(f\"\"\"\n", "        CREATE TABLE IF NOT EXISTS {full_table}\n", "        USING DELTA\n", "        LOCATION '{bronze_table_path}'\n", "    \"\"\")\n", "\n", "    archive_file_path = file_path.replace(\"/raw/\", \"/archive/\")\n", "    move_to_archive_s3(file_path, archive_file_path)\n", "\n", "# -------------------------\n", "# Step 3: Start Auto Loader\n", "# -------------------------\n", "(\n", "    df_with_file.writeStream\n", "    .foreachBatch(process_and_archive)\n", "    .option(\"checkpointLocation\", checkpoint_path + \"stream/\")\n", "    .start()\n", ")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 2}