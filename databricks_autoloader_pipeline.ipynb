{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39260629",
   "metadata": {},
   "source": [
    "# üì• Databricks Auto Loader Pipeline with Archiving to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389f950",
   "metadata": {},
   "source": [
    "\n",
    "This notebook implements a complete Databricks Auto Loader pipeline using PySpark.\n",
    "It ingests JSON files from an S3 bucket into a Delta Lake Bronze table, and moves the processed files\n",
    "to an archive location within the same bucket, grouped by prefix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "source_path = \"s3://your-bucket-name/data/\"\n",
    "bronze_table_path = \"/mnt/bronze/events/\"\n",
    "checkpoint_path = \"/mnt/bronze/checkpoints/events/\"\n",
    "archive_path = \"s3://your-bucket-name/archive/\"\n",
    "file_format = \"json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e218f1",
   "metadata": {},
   "source": [
    "## üîÅ Step 1: Read streaming data from S3 using Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Define schema explicitly (optional)\n",
    "schema = StructType().add(\"id\", StringType()).add(\"value\", StringType())\n",
    "\n",
    "# Load streaming data from S3\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", file_format)\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(source_path)\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05383682",
   "metadata": {},
   "source": [
    "## üíæ Step 2: Write to Bronze Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2561dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write streaming data to Delta table\n",
    "(\n",
    "    df.writeStream.format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start(bronze_table_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a9a35",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 3: Archive processed files by prefix using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b13a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to archive files by prefix\n",
    "def archive_processed_files(bucket: str, source_prefix: str, archive_prefix: str):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=source_prefix)\n",
    "    if \"Contents\" not in response:\n",
    "        print(\"No files to archive.\")\n",
    "        return\n",
    "\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        filename = os.path.basename(key)\n",
    "        prefix = filename.split(\"_\")[0] if \"_\" in filename else \"misc\"\n",
    "        dest_key = f\"{archive_prefix}{prefix}/{filename}\"\n",
    "\n",
    "        # Copy then delete\n",
    "        s3.copy_object(Bucket=bucket, CopySource={\"Bucket\": bucket, \"Key\": key}, Key=dest_key)\n",
    "        s3.delete_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Moved {key} ‚Üí {dest_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fbefc0",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 4: Call the archive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set S3 bucket and prefixes\n",
    "bucket = \"your-bucket-name\"\n",
    "source_prefix = \"data/\"\n",
    "archive_prefix = \"archive/\"\n",
    "\n",
    "# Archive processed files\n",
    "archive_processed_files(bucket, source_prefix, archive_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d01c5d",
   "metadata": {},
   "source": [
    "## üß™ (Optional) Step 5: Simulate sample ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8237399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use dbutils.fs.cp or upload a file manually to your source_path to test the flow."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
