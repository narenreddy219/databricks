{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e6f9cc",
   "metadata": {},
   "source": [
    "# üß™ Databricks Auto Loader\n",
    "This notebook scans S3 for files containing `edm_entity`, matches them to an existing Delta table, and loads them with format inference. If the target table doesn't exist, the file is left in the landing zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d6b34",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Configuration\n",
    "Set up base variables for schema name, organization, and S3 path locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "import os\n",
    "\n",
    "org_name = \"entity\"\n",
    "schema_name = \"edm\"\n",
    "\n",
    "landing_path = \"s3://your-bucket/landing-zone/\"\n",
    "router_schema_path = f\"s3://your-bucket/schema-tracking/__router__/\"\n",
    "router_checkpoint_path = f\"s3://your-bucket/checkpoints/__router__/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386f5ae",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: File Detection Stream\n",
    "Use Auto Loader to detect file arrivals in `landing_path` using binary format (only detects files, not content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b82665",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", router_schema_path)\n",
    "    .load(landing_path)\n",
    "    .withColumn(\"source_file\", input_file_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da67888",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Batch Processing Logic\n",
    "Loop over each file path and determine the correct routing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    paths = batch_df.select(\"path\").rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path)\n",
    "        ext = os.path.splitext(filename)[1].lower().strip(\".\")\n",
    "\n",
    "        if \"edm_entity\" in filename.lower():\n",
    "            table_name = \"edm_entity\"\n",
    "        else:\n",
    "            print(f\"‚ùå No routing rule for file: {filename}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0871c",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 4: Build Target Paths\n",
    "Construct target table name and tracking/checkpoint paths dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        target_table = f\"{org_name}.bronze.{table_name}\"\n",
    "        schema_tracking_path = f\"s3://your-bucket/schema-tracking/{org_name}/{schema_name}/{table_name}/\"\n",
    "        checkpoint_path = f\"s3://your-bucket/checkpoints/{org_name}/{schema_name}/{table_name}/\"\n",
    "        print(f\"üîç Routing file: {filename} ‚Üí Table: {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e1abe",
   "metadata": {},
   "source": [
    "## üì• Step 5: Read and Validate File Format\n",
    "Load the file based on its extension. Skip unsupported formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        try:\n",
    "            if ext in [\"csv\", \"txt\"]:\n",
    "                df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path)\n",
    "            elif ext == \"json\":\n",
    "                df = spark.read.option(\"inferSchema\", \"true\").json(path)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Unsupported format: {filename}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc7d74",
   "metadata": {},
   "source": [
    "## üß± Step 6: Validate Table Existence\n",
    "Check if the Delta table exists before writing. If not, leave the file unprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if not spark._jsparkSession.catalog().tableExists(target_table):\n",
    "                print(f\"üö´ Table not found for {filename}. File remains in landing zone.\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef51168",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Append Data to Table\n",
    "Write the DataFrame into the matched Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48372fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "            df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "            print(f\"‚úÖ Loaded into: {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766599af",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Step 8: Archive Processed File\n",
    "Move the file from landing to archive only after successful ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "            archive_path = path.replace(\"landing-zone\", \"archive-zone\")\n",
    "            dbutils.fs.mv(path, archive_path)\n",
    "            print(f\"üì¶ Archived: {archive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2015cc8",
   "metadata": {},
   "source": [
    "## ‚ùå Step 9: Handle Errors Gracefully\n",
    "Catch exceptions and continue with other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316f63f",
   "metadata": {},
   "source": [
    "## üöÄ Step 10: Start Stream\n",
    "Run the Auto Loader stream once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(raw_df.writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"checkpointLocation\", router_checkpoint_path)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    "    .awaitTermination())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
