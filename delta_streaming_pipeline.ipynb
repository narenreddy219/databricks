{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215ebf43",
   "metadata": {},
   "source": [
    "# üß™ Delta Lake Streaming Pipeline\n",
    "This notebook demonstrates a structured data ingestion pipeline using Apache Spark Structured Streaming.\n",
    "- Reads data from S3\n",
    "- Writes to Delta Lake tables\n",
    "- Supports merge/upsert and overwrite strategies\n",
    "- Includes validation and performance logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Optional\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('DeltaIngest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95287f18",
   "metadata": {},
   "source": [
    "## üîß Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a52c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamConfig:\n",
    "    def __init__(self):\n",
    "        self.catalog = \"hive_metastore\"\n",
    "        self.schema = \"bronze\"\n",
    "        self.table_name = \"entity_table\"\n",
    "        self.source_path = \"s3a://your-bucket/entity-data/\"\n",
    "        self.checkpoint_path = \"s3a://your-bucket/checkpoints/entity\"\n",
    "        self.merge_keys = [\"entity_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e89876",
   "metadata": {},
   "source": [
    "## üöÄ Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09805c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DeltaLakeIngest\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7fbde",
   "metadata": {},
   "source": [
    "## ‚úÖ Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self, merge_keys: List[str]):\n",
    "        self.merge_keys = merge_keys\n",
    "\n",
    "    def validate(self, df: DataFrame) -> bool:\n",
    "        for key in self.merge_keys:\n",
    "            if df.filter(df[key].isNull() | (df[key] == '')).count() > 0:\n",
    "                logger.warning(f\"Primary key '{key}' has null or empty values\")\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1392e52",
   "metadata": {},
   "source": [
    "## üîÅ Merge Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_delta(spark: SparkSession, df: DataFrame, config: StreamConfig):\n",
    "    df.createOrReplaceTempView(\"source\")\n",
    "    merge_condition = \" AND \".join([f\"target.{k} = source.{k}\" for k in config.merge_keys])\n",
    "    merge_sql = f\"\"\"\n",
    "        MERGE INTO {config.catalog}.{config.schema}.{config.table_name} AS target\n",
    "        USING source\n",
    "        ON {merge_condition}\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19b1d4",
   "metadata": {},
   "source": [
    "## üìå Main Ingestion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7784b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    schema = StructType([\n",
    "        StructField(\"entity_id\", StringType(), True),\n",
    "        StructField(\"entity_name\", StringType(), True),\n",
    "        StructField(\"entity_code\", StringType(), True),\n",
    "        StructField(\"as_of_date\", StringType(), True)\n",
    "    ])\n",
    "    config = StreamConfig()\n",
    "    spark = setup_spark()\n",
    "    df = spark.readStream.option(\"header\", \"true\").schema(schema).csv(config.source_path)\n",
    "    validator = DataValidation(config.merge_keys)\n",
    "    df_validated = df.filter(lambda d: validator.validate(d))\n",
    "    query = df_validated.writeStream.format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", config.checkpoint_path) \\\n",
    "        .toTable(f\"{config.catalog}.{config.schema}.{config.table_name}\")\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1341d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run\n# main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
