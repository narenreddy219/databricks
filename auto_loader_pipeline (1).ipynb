{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Auto Loader Pipeline: Dynamic Schema Ingestion by Entity"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# =============================================\n", "# \ud83d\udcc1 CONFIGURATION\n", "# =============================================\n", "\n", "config = {\n", "    \"s3_bucket\": \"your-s3-bucket-name\",\n", "    \"landing_prefix\": \"bronze/landing/\",\n", "    \"archive_prefix\": \"bronze/archive/\",\n", "    \"bronze_schema\": \"bronze\",\n", "    \"entity_regex\": r\"edm_(entity[a-zA-Z0-9]*)\",\n", "    \"file_format\": \"auto\",\n", "    \"file_header\": \"true\",\n", "    \"checkpoint_path\": \"dbfs:/mnt/bronze/checkpoints/edm_entity\"\n", "}\n", "\n", "config[\"source_path\"] = f\"s3a://{config['s3_bucket']}/{config['landing_prefix']}\"\n", "config[\"archive_path\"] = f\"s3a://{config['s3_bucket']}/{config['archive_prefix']}\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2699\ufe0f IMPORTS\n", "\n", "from pyspark.sql.functions import input_file_name, regexp_extract\n", "from pyspark.sql.utils import AnalysisException"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2699\ufe0f LOAD STREAM FROM S3 USING AUTO LOADER\n", "\n", "def load_stream(config):\n", "    return (\n", "        spark.readStream\n", "        .format(\"cloudFiles\")\n", "        .option(\"cloudFiles.format\", config[\"file_format\"])\n", "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n", "        .option(\"header\", config[\"file_header\"])\n", "        .option(\"cloudFiles.includeExistingFiles\", \"true\")\n", "        .option(\"cloudFiles.archiveDir\", config[\"archive_path\"])\n", "        .load(config[\"source_path\"])\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2699\ufe0f ENRICH DATA WITH ENTITY FROM FILENAME\n", "\n", "def enrich_with_entity(df, config):\n", "    df = df.withColumn(\"filename\", input_file_name())\n", "    df = df.withColumn(\"entity\", regexp_extract(\"filename\", config[\"entity_regex\"], 1))\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2699\ufe0f WRITE DATA TO ENTITY-SPECIFIC DELTA TABLES\n", "\n", "def write_entity_tables(bronze_schema):\n", "    def writer(batch_df, batch_id):\n", "        entity_names = batch_df.select(\"entity\").distinct().collect()\n", "\n", "        for row in entity_names:\n", "            entity = row[\"entity\"]\n", "            if not entity:\n", "                continue\n", "\n", "            table_name = f\"{bronze_schema}.{entity}\"\n", "            entity_df = batch_df.filter(batch_df[\"entity\"] == entity).drop(\"filename\", \"entity\")\n", "\n", "            try:\n", "                if not spark.catalog.tableExists(table_name):\n", "                    (\n", "                        entity_df.write\n", "                        .format(\"delta\")\n", "                        .mode(\"overwrite\")\n", "                        .option(\"overwriteSchema\", \"true\")\n", "                        .saveAsTable(table_name)\n", "                    )\n", "                else:\n", "                    (\n", "                        entity_df.write\n", "                        .format(\"delta\")\n", "                        .mode(\"append\")\n", "                        .saveAsTable(table_name)\n", "                    )\n", "            except AnalysisException as e:\n", "                print(f\"\u26a0\ufe0f Error writing {table_name}: {str(e)}\")\n", "    return writer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\ude80 START STREAMING INGESTION PIPELINE\n", "\n", "df_stream = load_stream(config)\n", "df_enriched = enrich_with_entity(df_stream, config)\n", "\n", "query = (\n", "    df_enriched.writeStream\n", "    .foreachBatch(write_entity_tables(config[\"bronze_schema\"]))\n", "    .option(\"checkpointLocation\", config[\"checkpoint_path\"])\n", "    .start()\n", ")\n", "\n", "query.awaitTermination()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}