{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53263984",
   "metadata": {},
   "source": [
    "# üìò Dynamic Auto Loader Project\n",
    "This notebook ingests files like `edm_entity1.txt`, `edm_sub1.txt` from S3 and dynamically routes them to Delta tables based on filename. Each table will be auto-created inside its respective schema if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99363549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 1: Setup\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col\n",
    "import re\n",
    "\n",
    "# üìç Organization and S3 config\n",
    "org_name = \"entity_resolution_dev\"\n",
    "bucket = \"your-s3-bucket\"\n",
    "s3_base_input = f\"s3://{bucket}/autoloader-input/{org_name}/\"\n",
    "s3_base_output = f\"s3://{bucket}/delta/{org_name}/\"\n",
    "\n",
    "# üìç Watch all files in org folder\n",
    "df = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"text\")\n",
    "      .load(s3_base_input)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 2: Parse Schema and Table Name From File Name\n",
    "df = df.withColumn(\"source_file\", input_file_name())\n",
    "df = df.withColumn(\"file_base\", regexp_extract(\"source_file\", r\"([^/]+)\\.txt$\", 1))\n",
    "df = df.withColumn(\"schema_name\", regexp_extract(\"file_base\", r\"([a-zA-Z_]+)[0-9a-z]*\", 1))\n",
    "df = df.withColumn(\"database\", col(\"schema_name\"))\n",
    "df = df.withColumn(\"table\", col(\"schema_name\"))\n",
    "\n",
    "df.select(\"source_file\", \"file_base\", \"database\", \"table\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 3: Auto Create Schema/Table Function\n",
    "def ensure_schema_table(df_sample, database, table, output_path):\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {database}\")\n",
    "    full_table_name = f\"{database}.{table}\"\n",
    "\n",
    "    if not spark.catalog.tableExists(full_table_name):\n",
    "        print(f\"üÜï Creating table: {full_table_name}\")\n",
    "        df_sample.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {full_table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{output_path}'\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Table {full_table_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Step 4: Split by Schema/Table and Write Stream to Correct Destination\n",
    "schema_table_pairs = df.select(\"database\", \"table\").distinct().collect()\n",
    "\n",
    "for row in schema_table_pairs:\n",
    "    database = row[\"database\"]\n",
    "    table = row[\"table\"]\n",
    "    output_path = f\"{s3_base_output}{database}/{table}/\"\n",
    "    filtered_df = df.filter((col(\"database\") == database) & (col(\"table\") == table))\n",
    "\n",
    "    parsed_df = filtered_df.selectExpr(\"split(value, ',') as columns\")\\\n",
    "                           .selectExpr(\"columns[0] as col1\", \"columns[1] as col2\")\n",
    "\n",
    "    ensure_schema_table(parsed_df.limit(1), database, table, output_path)\n",
    "\n",
    "    (parsed_df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{output_path}/_checkpoint\")\n",
    "        .start(output_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}