{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e2b1ee",
   "metadata": {},
   "source": [
    "# 🔐 Databricks Auto Loader with Temporary AWS Credentials and Archiving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10938e7f",
   "metadata": {},
   "source": [
    "\n",
    "This notebook implements a secure and modular pipeline in Databricks that:\n",
    "- Fetches temporary AWS credentials from a custom identity service\n",
    "- Injects them into Spark config for S3 Auto Loader access\n",
    "- Streams data into a Bronze Delta table using Auto Loader\n",
    "- Archives processed files using `boto3`\n",
    "- Supports credential refresh for long-running jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61c7dd",
   "metadata": {},
   "source": [
    "## 🔧 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6bc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Identity service configuration\n",
    "identity_url = \"https://epi.v4ic-identity.ssov.factset.com/creds\"\n",
    "aws_username = dbutils.secrets.get(\"my_scope\", \"aws_username\")\n",
    "aws_password = dbutils.secrets.get(\"my_scope\", \"aws_password\")\n",
    "aws_account_id = \"123456789012\"\n",
    "aws_role_name = \"my-role\"\n",
    "ca_bundle_file = \"/dbfs/path/to/ca.pem\"  # Optional\n",
    "\n",
    "# S3 paths\n",
    "bucket = \"your-bucket\"\n",
    "source_prefix = \"input/\"\n",
    "archive_prefix = \"archive/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96884d8",
   "metadata": {},
   "source": [
    "## 🔑 Helper: Fetch Temporary AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74953a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_temp_aws_creds():\n",
    "    payload = {\n",
    "        \"aws_username\": aws_username,\n",
    "        \"aws_password\": aws_password,\n",
    "        \"aws_account_id\": aws_account_id,\n",
    "        \"aws_role_name\": aws_role_name,\n",
    "    }\n",
    "    response = requests.post(identity_url, json=payload, verify=ca_bundle_file)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    creds = {\n",
    "        \"AWS_ACCESS_KEY_ID\": data[\"awsAccessKey\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": data[\"awsSecretKey\"],\n",
    "        \"AWS_SESSION_TOKEN\": data[\"awsSessionToken\"],\n",
    "        \"AWS_EXPIRATION\": data[\"awsExpires\"],\n",
    "    }\n",
    "    return creds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c4d59",
   "metadata": {},
   "source": [
    "## 🚀 Inject Credentials into Spark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inject_aws_creds_into_spark(creds: dict):\n",
    "    spark.conf.set(\"fs.s3a.access.key\", creds[\"AWS_ACCESS_KEY_ID\"])\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", creds[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "    spark.conf.set(\"fs.s3a.session.token\", creds[\"AWS_SESSION_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a6abf",
   "metadata": {},
   "source": [
    "## ♻️ Credential Refresh Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aws_creds = get_temp_aws_creds()\n",
    "inject_aws_creds_into_spark(aws_creds)\n",
    "\n",
    "def is_token_expired(creds: dict, buffer_minutes=5):\n",
    "    expiry = datetime.strptime(creds[\"AWS_EXPIRATION\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return datetime.utcnow() + timedelta(minutes=buffer_minutes) > expiry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459553b8",
   "metadata": {},
   "source": [
    "## 📥 Read From S3 Using Auto Loader with Temporary Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02393ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Optional schema\n",
    "schema = StructType().add(\"id\", StringType()).add(\"value\", StringType())\n",
    "\n",
    "source_path = f\"s3a://{bucket}/{source_prefix}\"\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(source_path)\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e86dab",
   "metadata": {},
   "source": [
    "## 💾 Write to Bronze Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6936780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = \"/mnt/bronze/checkpoints/\"\n",
    "bronze_table_path = \"/mnt/bronze/data/\"\n",
    "\n",
    "(\n",
    "    df.writeStream.format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start(bronze_table_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e704e7",
   "metadata": {},
   "source": [
    "## 📦 Archive Processed Files Using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57959a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def archive_files(creds, bucket, source_prefix, archive_prefix):\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=creds[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=creds[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        aws_session_token=creds[\"AWS_SESSION_TOKEN\"],\n",
    "    )\n",
    "\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=source_prefix)\n",
    "    for obj in resp.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        filename = os.path.basename(key)\n",
    "        file_prefix = filename.split(\"_\")[0]\n",
    "        dest_key = f\"{archive_prefix}{file_prefix}/{filename}\"\n",
    "        s3.copy_object(Bucket=bucket, CopySource={\"Bucket\": bucket, \"Key\": key}, Key=dest_key)\n",
    "        s3.delete_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Archived {key} → {dest_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cc507",
   "metadata": {},
   "source": [
    "## ✅ Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ec0ec",
   "metadata": {},
   "source": [
    "\n",
    "- Temporary AWS credentials securely retrieved from identity service\n",
    "- Injected into Spark config for Auto Loader to access S3\n",
    "- Optional refresh logic checks if token is near expiry\n",
    "- Files are archived to a prefix-based S3 structure using `boto3`\n",
    "\n",
    "> You can modularize these helpers into a shared module for reuse across notebooks or jobs.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
